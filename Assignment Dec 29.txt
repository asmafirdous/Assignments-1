Make short notes on Gradient Clipping, Cross Entropy

What is gradient clipping? 
Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation 
through the network, and using the clipped gradients to update the weights.

Why is gradient clipping used?
Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. 
A neural network is a learning algorithm, also called neural network or neural net, 
that uses a network of functions to understand and translate data input into a specific output.

Cross-entropy:
Cross-entropy measures the performance of a classification model based on the probability and error, 
where the more likely (or the bigger the probability) of something is, the lower the cross-entropy.

What is cross-entropy formula?
Cross-entropy can be calculated using the probabilities of the events from P and Q, 
as follows: H(P, Q) = â€” sum x in X P(x) * log(Q(x))


What is entropy and cross-entropy?
Cross entropy is the average number of bits used to transfer the information. 
The cross entropy is always less than or equal to the entropy. For the random user predicting machine, 
the number of bits used to transfer the information is 1 so the cross entropy is 1

What is log loss and cross-entropy?
Log Loss (Binary Cross-Entropy Loss): A loss function that represents how much the predicted probabilities deviate from the true ones.
It is used in binary cases. Cross-Entropy Loss: A generalized form of the log loss, which is used for multi-class classification problems.

